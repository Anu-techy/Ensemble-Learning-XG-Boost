**XG-Boost** is a A powerful, fast, and regularized version of Gradient Boosting.


**Strengths**

handles complex and non-minear r/ps well

Feature importance and selection

Speed & Performance: It's highly optimized for speed (C++ backend).

Regularization: L1 & L2 regularization to avoid overfitting.

Handles Missing Data: Smart handling of missing values during training.

Parallel Processing: Unlike traditional GBM, XGBoost can train in parallel.

Cross-validation built-in: Native support for k-fold CV and early stopping.

**Reasons for high accuracy:**

tree based and boosting approach

Tree pruning (gamma) 

parallel processing

cache awareness

Efficient handling of Sparse data

**Can be used for:**

1. Regression
2. Classification
3. Ranking
4. Time series (with feature engineering)

